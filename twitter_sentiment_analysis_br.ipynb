{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_sentiment_analysis_br.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pcampos119104/twitter-sentiment-analysis-br/blob/master/twitter_sentiment_analysis_br.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlaC3Mt_H4xJ",
        "colab_type": "text"
      },
      "source": [
        "# TODOs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6TQszmAIIzK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*  Pep 8 \n",
        "*  How to access a directory on GCP Bucket\n",
        "*  How to update tensorflow to 2.x without breaking bert-tf\n",
        "*  why  tf.keras.utils.get_file it's not extracting the zip file?\n",
        "*  Use that to split the data from sklearn.model_selection import train_test_splir\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdojI3ryD1HN",
        "colab_type": "text"
      },
      "source": [
        "# Imports and installations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVR25JihFjQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-1wakFVmb4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "try:\n",
        "  # %tensorflow_version only exists on colab\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  print('exception')\n",
        "  pass\n",
        "'''\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import bert \n",
        "from bert import run_classifier\n",
        "from bert import optimization \n",
        "from bert.tokenization import FullTokenizer\n",
        "from bert.modeling import BertConfig, BertModel\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvWEU-B5GlrK",
        "colab_type": "text"
      },
      "source": [
        "# Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iNEhmbuV49H",
        "colab_type": "text"
      },
      "source": [
        "Config GCP Bucket\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDmMvhcBWI_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://cloud.google.com/storage/docs/downloading-objects#storage-download-object-python\n",
        "\n",
        "def download_blob(project_id, bucket_name, source_blob_name) -> None:\n",
        "    \"\"\"Downloads a blob from the GCP bucket.\n",
        "    \n",
        "    Args:\n",
        "     project_id: Project id on GCP\n",
        "     bucket_name: Your bucket name\n",
        "     source_blob_name: Storage object name\n",
        "    \n",
        "    Returns:\n",
        "     None\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    storage_client = storage.Client(project_id)\n",
        "\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "    blob.download_to_filename(source_blob_name)\n",
        "\n",
        "    print(\n",
        "        \"Blob {} downloaded.\".format(\n",
        "            source_blob_name\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuU92-xSGOzj",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Set the output directory for saving model file\n",
        "# Optionally, set a GCP bucket location\n",
        "\n",
        "\n",
        "#Project id on GCP \n",
        "PROJECT_ID = 'dl-sentimentbr-119104'\n",
        "\n",
        "#Set the data directory name on GCP bucket.\n",
        "INPUT_DIR = 'data'\n",
        "#Set the name of the file with the data \n",
        "FILE_NAME = \"tweets_dl_sentimentbr.csv\" \n",
        "#Whether or not need to extract the file \n",
        "DO_EXTRACT= False \n",
        "\n",
        "#Set the working directory name on GCP bucket.\n",
        "OUTPUT_DIR = 'dl_sentimentbr'\n",
        "#Whether or not to clear/delete the directory and create a new one\n",
        "DO_DELETE = True \n",
        "#Set bucket name to retrieve dataset and save model.\n",
        "BUCKET = 'pcampos119104' \n",
        "\n",
        "OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
        "INPUT_DIR = 'gs://{}/{}'.format(BUCKET, INPUT_DIR)\n",
        "auth.authenticate_user()\n",
        "\n",
        "if DO_DELETE:\n",
        "  try:\n",
        "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "  except:\n",
        "    # Doesn't matter if the directory didn't exist\n",
        "    pass\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('Model output directory: {}'.format(OUTPUT_DIR))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYgWOFTxT5JB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "download_blob(PROJECT_ID, BUCKET, FILE_NAME)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUHVZVIop-Fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(FILE_NAME)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z30bAuZatCZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPa7m3UXtKrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ1NTa3YU2xQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(train_size, dataframe):\n",
        "  \"\"\"Split the data in train, dev and test, the proportion depends\n",
        "  on the train_size.\n",
        "\n",
        "  Args:\n",
        "    data_size: how many examples to use from dataset.\n",
        "    dataframe: the dataset to be sliced.\n",
        "  Returns:\n",
        "    TODO\n",
        "  \"\"\"\n",
        "  if train_size < 400000:\n",
        "    ntrain = int(train_size * 0.6)\n",
        "    ndev_test = int(train_size * 0.2)\n",
        "  else:\n",
        "    ntrain = int(train_size * 0.98)\n",
        "    ndev_test = int(train_size * 0.01)\n",
        "  \n",
        "  data_train = dataframe[:ntrain]\n",
        "  data_dev = dataframe[ntrain:ntrain + ndev_test]\n",
        "  data_test = dataframe[ntrain + ndev_test:ntrain + ndev_test*2]\n",
        "\n",
        "  return data_train, data_dev, data_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0WQvEQnU2aW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = 500000\n",
        "\n",
        "data_train, data_dev, data_test = split_data(train_size, df)\n",
        "print(data_train.count())\n",
        "print(data_dev.count())\n",
        "print(data_test.count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQr4veH_Lwgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(data_dev['tweet_text'].where(data_dev['tweet_text'].values==data_test['tweet_text'].values).notna().unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWDj9MMR2oe3",
        "colab_type": "text"
      },
      "source": [
        "# Building and exporting a tf.Module "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rdKlAFQFg2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO Refactor to a function\n",
        "MODEL_URL = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/bert-large-portuguese-cased_tensorflow_checkpoint.zip\"\n",
        "MODEL_PATH_AND_NAME = \"/content/model.zip\"\n",
        "\n",
        "VOCAB_URL = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/vocab.txt\"\n",
        "VOCAB_PATH_AND_NAME = \"/content/vocab.txt\"\n",
        "\n",
        "model = tf.keras.utils.get_file(\n",
        "    fname=MODEL_PATH_AND_NAME, \n",
        "    origin=MODEL_URL, \n",
        "    extract=True)\n",
        "\n",
        "vocab_path = tf.keras.utils.get_file(\n",
        "    fname=VOCAB_PATH_AND_NAME, \n",
        "    origin=VOCAB_URL)\n",
        "\n",
        "\n",
        "!unzip {model}\n",
        "!rm -r {model}\n",
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOyKrgZRRqZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Greate code copied from https://colab.research.google.com/drive/1ofSfThTBlWjOx5dqXmdsIol-MdiqCyZC#scrollTo=cOyKrgZRRqZe\n",
        "def build_module_fn(config_path, vocab_path, do_lower_case=True):\n",
        "\n",
        "    def bert_module_fn(is_training):\n",
        "        \"\"\"Spec function for a token embedding module.\"\"\"\n",
        "\n",
        "        input_ids = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_ids\")\n",
        "        input_mask = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_mask\")\n",
        "        token_type = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "        config = BertConfig.from_json_file(config_path)\n",
        "        model = BertModel(config=config, \n",
        "                          is_training=is_training,\n",
        "                          input_ids=input_ids, \n",
        "                          input_mask=input_mask, \n",
        "                          token_type_ids=token_type)\n",
        "          \n",
        "        seq_output = model.all_encoder_layers[-1]\n",
        "        pool_output = model.get_pooled_output()\n",
        "\n",
        "        config_file = tf.constant(value=config_path, dtype=tf.string, name=\"config_file\")\n",
        "        vocab_file = tf.constant(value=vocab_path, dtype=tf.string, name=\"vocab_file\")\n",
        "        lower_case = tf.constant(do_lower_case)\n",
        "\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, config_file)\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, vocab_file)\n",
        "        \n",
        "        input_map = {\"input_ids\": input_ids,\n",
        "                     \"input_mask\": input_mask,\n",
        "                     \"segment_ids\": token_type}\n",
        "        \n",
        "        output_map = {\"pooled_output\": pool_output,\n",
        "                      \"sequence_output\": seq_output}\n",
        "\n",
        "        output_info_map = {\"vocab_file\": vocab_file,\n",
        "                           \"do_lower_case\": lower_case}\n",
        "                \n",
        "        hub.add_signature(name=\"tokens\", inputs=input_map, outputs=output_map)\n",
        "        hub.add_signature(name=\"tokenization_info\", inputs={}, outputs=output_info_map)\n",
        "\n",
        "    return bert_module_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4fShtfnSQbO",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Greate code copied from https://colab.research.google.com/drive/1ofSfThTBlWjOx5dqXmdsIol-MdiqCyZC#scrollTo=cOyKrgZRRqZe\n",
        "config_path = \"/content/bert_config.json\"\n",
        "vocab_path = \"/content/vocab.txt\"\n",
        "\n",
        "tags_and_args = []\n",
        "for is_training in (True, False):\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  tags_and_args.append((tags, dict(is_training=is_training)))\n",
        "module_fn = build_module_fn(config_path, vocab_path, do_lower_case=False)\n",
        "spec = hub.create_module_spec(module_fn, tags_and_args=tags_and_args)\n",
        "spec.export(\"bert-module\", \n",
        "            checkpoint_path=\"/content/model.ckpt-1000000\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilAnpxPP8d7-",
        "colab_type": "text"
      },
      "source": [
        "# Using "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsszxofSX24_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BERT_MODEL_HUB = \"/content/bert-module\"\n",
        "# TODO Abstract more to receive https or gs path\n",
        "def create_tokenizer_from_hub_module(model_hub) -> FullTokenizer:\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    \n",
        "    bert_module = hub.Module(os.path.abspath(model_hub))\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "\n",
        "  return FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module(BERT_MODEL_HUB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALkyk1NGdezT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.tokenize(data_dev.iloc[3]['tweet_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rplDh_bmGuQQ",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_J8wLlNONmL",
        "colab_type": "text"
      },
      "source": [
        "Transform the data to input on bert model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJZC_eZ3OdI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRkg04iiPScs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'tweet_text'\n",
        "LABEL_COLUMN = 'sentiment'\n",
        "label_list = [0, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKniQY7nQbfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_InputExamples = data_train.apply(\n",
        "    lambda x: bert.run_classifier.InputExample(\n",
        "        guid=None, \n",
        "        text_a = x[DATA_COLUMN], \n",
        "        text_b = None, \n",
        "        label = x[LABEL_COLUMN]),\n",
        "        axis = 1)\n",
        "dev_InputExamples = data_dev.apply(\n",
        "    lambda x: bert.run_classifier.InputExample(\n",
        "        guid=None,\n",
        "        text_a = x[DATA_COLUMN],\n",
        "        text_b = None,\n",
        "        label = x[LABEL_COLUMN]),\n",
        "        axis = 1)\n",
        "test_InputExamples = data_test.apply(\n",
        "    lambda x: bert.run_classifier.InputExample(\n",
        "        guid=None,\n",
        "        text_a = x[DATA_COLUMN],\n",
        "        text_b = None,\n",
        "        label = x[LABEL_COLUMN]),\n",
        "        axis = 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUaw29ZUEjW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll set sequences to be at most 64 tokens long.\n",
        "MAX_SEQ_LENGTH = 64\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "dev_features = bert.run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYbB7Pn7G1vt",
        "colab_type": "text"
      },
      "source": [
        "# Creating a model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRl-sbu0jC3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1B743OflE8t",
        "colab_type": "text"
      },
      "source": [
        "# Building the preprocessing text pipeline "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btdXAInalSJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOmTh1gfHF02",
        "colab_type": "text"
      },
      "source": [
        "# Predicting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNZWNd5dAYdy",
        "colab_type": "text"
      },
      "source": [
        "# Publishing \n"
      ]
    }
  ]
}